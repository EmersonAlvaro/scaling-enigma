{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-01-19T23:34:31.653096Z","iopub.status.busy":"2022-01-19T23:34:31.652255Z","iopub.status.idle":"2022-01-19T23:34:31.668796Z","shell.execute_reply":"2022-01-19T23:34:31.667465Z","shell.execute_reply.started":"2022-01-19T23:34:31.653020Z"},"trusted":true},"outputs":[],"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import tensorflow as tf\n","import numpy as np\n","import cv2 as cv\n","import pandas as pd\n","import os\n","import pathlib\n","import sys\n","import gc\n","import numpy as np\n","import random\n","import shutil\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","from tensorflow.keras import regularizers\n","# from tensorflow.keras.utils import np_utils\n","from sklearn import metrics\n","from tensorflow.keras.utils import plot_model\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.losses import sparse_categorical_crossentropy\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing import image_dataset_from_directory"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:34:31.671968Z","iopub.status.busy":"2022-01-19T23:34:31.670993Z","iopub.status.idle":"2022-01-19T23:34:31.689203Z","shell.execute_reply":"2022-01-19T23:34:31.688453Z","shell.execute_reply.started":"2022-01-19T23:34:31.671918Z"},"trusted":true},"outputs":[],"source":["IMAGE_SIZE = 256\n","train_data = pathlib.Path('/kaggle/input/.../data/train/')\n","test_data = pathlib.Path('/kaggle/input/.../data/test/')\n","output = pathlib.Path('/kaggle/working/')\n","\n","num_class = 4\n","\n","epoch = 70\n","loss_function = sparse_categorical_crossentropy\n","# optimizer = RMSprop ()\n","optimizer = Adam(learning_rate=1e-5)\n","# optimizer = SGD()\n","num_folds = 10\n","batchsize = 64\n","\n","# container for metrics\n","acc_folds = []\n","f1_folds = []\n","prec_folds = []\n","recall_folds = []"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:34:31.691791Z","iopub.status.busy":"2022-01-19T23:34:31.690703Z","iopub.status.idle":"2022-01-19T23:34:31.705562Z","shell.execute_reply":"2022-01-19T23:34:31.704502Z","shell.execute_reply.started":"2022-01-19T23:34:31.691737Z"},"trusted":true},"outputs":[],"source":["def resize_and_rescale(image):\n","    image = tf.cast(image, tf.float32)\n","    image = (image / 255.0)\n","    return image\n","\n","def load_data(data_directory):\n","    directories = [d for d in os.listdir(data_directory)\n","                   if os.path.isdir(os.path.join(data_directory, d))]\n","    labels = []\n","    images = []\n","    file_names = []\n","    for d in directories:\n","        label_directory = os.path.join(data_directory, d)\n","        file_names += [os.path.join(label_directory, f)\n","                       for f in os.listdir(label_directory)]\n","\n","    random.shuffle(file_names)\n","\n","    for f in file_names:\n","        img = cv.imread(f)\n","        img = resize_and_rescale(img)\n","        dirname = os.path.split(os.path.dirname(f))[1]\n","\n","        images.append(img)\n","        labels.append(int(dirname))       \n","\n","    images, labels = np.asarray(images), np.asarray(labels)\n","\n","    return images, labels\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:34:31.750672Z","iopub.status.busy":"2022-01-19T23:34:31.750206Z","iopub.status.idle":"2022-01-19T23:34:31.787104Z","shell.execute_reply":"2022-01-19T23:34:31.786042Z","shell.execute_reply.started":"2022-01-19T23:34:31.750622Z"},"trusted":true},"outputs":[],"source":["def xception(num_class):\n","    stride = (2, 2)\n","    kernel_size = (3, 3)\n","    pool_size = (2, 2)\n","    Channel_axis = 3\n","\n","    def entry_flow(img_input):\n","\n","\n","        x = layers.Conv2D(32, kernel_size=kernel_size,\n","                          use_bias=False, kernel_initializer='glorot_uniform',\n","                          strides=stride, padding='SAME')(img_input)\n","        x = layers.BatchNormalization(axis=Channel_axis)(x)\n","        x = LeakyReLU(alpha=0.1)(x)\n","\n","        x = layers.Conv2D(64, kernel_size=kernel_size,\n","                          use_bias=False, kernel_initializer='glorot_uniform',\n","                          padding='SAME')(x)\n","        x = layers.BatchNormalization(axis=Channel_axis)(x)\n","        x = LeakyReLU(alpha=0.1)(x)\n","\n","        x_temp = x\n","\n","        for filter in [128, 256, 728]:\n","            if filter != 128:\n","                x = LeakyReLU(alpha=0.1)(x)\n","            x = layers.SeparableConv2D(filters=filter, kernel_size=kernel_size,\n","                                       use_bias=False, kernel_initializer='glorot_uniform',\n","                                       padding='SAME')(x)\n","            x = layers.BatchNormalization(axis=Channel_axis)(x)\n","\n","            x = LeakyReLU(alpha=0.1)(x)\n","            x = layers.SeparableConv2D(filters=filter, kernel_size=kernel_size,\n","                                       use_bias=False, kernel_initializer='glorot_uniform',\n","                                       padding='SAME')(x)\n","            x = layers.BatchNormalization(axis=Channel_axis)(x)\n","\n","            x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n","\n","            x_shortcut = layers.Conv2D(filters=filter, kernel_size=(1, 1),\n","                                       use_bias=False, kernel_initializer='glorot_uniform',\n","                                       strides=stride, padding='SAME')(x_temp)\n","            x_shortcut = layers.BatchNormalization(axis=Channel_axis)(x_shortcut)\n","\n","            x = layers.add([x, x_shortcut])\n","            x_temp = x\n","\n","        return x\n","\n","    def middle_flow(x):\n","\n","        x_temp = x\n","\n","        for i in range(8):\n","            x = LeakyReLU(alpha=0.1)(x)\n","            x = layers.SeparableConv2D(filters=728, kernel_size=kernel_size,\n","                                       use_bias=False, kernel_initializer='glorot_uniform',\n","                                       padding='SAME')(x)\n","            x = layers.BatchNormalization(axis=Channel_axis)(x)\n","\n","            x = LeakyReLU(alpha=0.1)(x)\n","            x = layers.SeparableConv2D(filters=728, kernel_size=kernel_size,\n","                                       use_bias=False, kernel_initializer='glorot_uniform',\n","                                       padding='SAME')(x)\n","            x = layers.BatchNormalization(axis=Channel_axis)(x)\n","\n","            x = LeakyReLU(alpha=0.1)(x)\n","            x = layers.SeparableConv2D(filters=728, kernel_size=kernel_size,\n","                                       use_bias=False, kernel_initializer='glorot_uniform',\n","                                       padding='SAME')(x)\n","            x = layers.BatchNormalization(axis=Channel_axis)(x)\n","\n","            x = layers.add([x, x_temp])\n","            x_temp = x\n","\n","        return x\n","\n","    def exit_flow(x):\n","\n","        x_temp = x\n","\n","        x = LeakyReLU(alpha=0.1)(x)\n","        x = layers.SeparableConv2D(filters=728, kernel_size=kernel_size,\n","                                   use_bias=False, kernel_initializer='glorot_uniform',\n","                                   padding='SAME')(x)\n","        x = layers.BatchNormalization(axis=Channel_axis)(x)\n","\n","        x = LeakyReLU(alpha=0.1)(x)\n","        x = layers.SeparableConv2D(filters=1024, kernel_size=kernel_size,\n","                                   use_bias=False, kernel_initializer='glorot_uniform',\n","                                   padding='SAME')(x)\n","        x = layers.BatchNormalization(axis=Channel_axis)(x)\n","\n","        x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n","\n","        x_shortcut = layers.Conv2D(filters=1024, kernel_size=kernel_size,\n","                                   use_bias=False, kernel_initializer='glorot_uniform',\n","                                   strides=stride, padding='SAME')(x_temp)\n","        x_shortcut = layers.BatchNormalization(axis=Channel_axis)(x_shortcut)\n","\n","        x = layers.add([x, x_shortcut])\n","\n","        x = layers.SeparableConv2D(filters=1536, kernel_size=kernel_size,\n","                                   use_bias=False, kernel_initializer='glorot_uniform',\n","                                   padding='SAME')(x)\n","        x = layers.BatchNormalization(axis=Channel_axis)(x)\n","        x = LeakyReLU(alpha=0.1)(x)\n","\n","        x = layers.SeparableConv2D(filters=2048, kernel_size=kernel_size,\n","                                   use_bias=False, kernel_initializer='glorot_uniform',\n","                                   padding='SAME')(x)\n","        x = layers.BatchNormalization(axis=Channel_axis)(x)\n","        x = LeakyReLU(alpha=0.1)(x)\n","\n","        x = layers.GlobalAveragePooling2D()(x)\n","\n","        x = layers.Dropout(0.8)(x)\n","\n","        output = layers.Dense(num_class, activation=tf.nn.softmax, dtype=tf.float32)(x)\n","\n","        return output\n","\n","    img_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n","\n","    x = entry_flow(img_input)\n","    x = middle_flow(x)\n","    output = exit_flow(x)\n","\n","    model = models.Model(inputs=img_input, outputs=output, name='Xception')\n","\n","    return model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:34:31.789974Z","iopub.status.busy":"2022-01-19T23:34:31.789628Z","iopub.status.idle":"2022-01-20T00:23:43.326498Z","shell.execute_reply":"2022-01-20T00:23:43.325379Z","shell.execute_reply.started":"2022-01-19T23:34:31.789928Z"},"trusted":true},"outputs":[],"source":["images, labels = load_data(train_data)\n","\n","print(images.shape)\n","print(labels.shape)\n","\n","tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","tf.config.experimental_connect_to_cluster(tpu)\n","tf.tpu.experimental.initialize_tpu_system(tpu)\n","\n","# instantiate a distribution strategy\n","tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","\n","kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=7)\n","indice = 0\n","\n","for train, val in kf.split(images, labels):\n","#     imgs_train, imgs_val = images[train], images[test]\n","#     labs_train, labs_val = labels[train], labels[test]\n","    with tpu_strategy.scope():\n","    \n","        myXception = xception(num_class)\n","        myXception.compile(loss=loss_function,optimizer=optimizer,metrics=['accuracy'])\n","    \n","    xcept = myXception.fit(images[train],labels[train], epochs=epoch, batch_size= batchsize,   validation_data=(images[val],labels[val]) )\n","\n","    imgs_test, labs_test = load_data(test_data)   \n","\n","    print(imgs_test.shape)\n","    print(labs_test.shape)\n","\n","    labs_predict = myXception.predict(imgs_test)\n","\n","    labs_predict = np.argmax(labs_predict, axis=1)\n","\n","    accuracy = metrics.accuracy_score(labs_test, labs_predict)\n","    f1 = metrics.f1_score(labs_test, labs_predict, average=\"macro\")\n","    precision = metrics.precision_score(labs_test, labs_predict, average=\"macro\")\n","    recall = metrics.recall_score(labs_test, labs_predict, average=\"macro\")\n","    recall_folds.append(recall)\n","    f1_folds.append(f1)\n","    acc_folds.append(accuracy)\n","    prec_folds.append(precision)\n","\n","    print('Acurracy: %f' % accuracy)\n","    print('F1: %f' % f1)\n","    print('Precision: %f' % precision)\n","    print('Recall: %f' % recall)\n","  \n","    print(\"==========================================================================\")\n","    plt.figure(0)\n","    plt.plot(xcept.history['accuracy'], 'r', label=\"train_acc\")\n","    plt.plot(xcept.history['val_accuracy'], 'g', label=\"val_acc\")\n","    plt.xlabel(\"Num of Epochs\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.yscale('linear')\n","    plt.xscale('linear')\n","    plt.title(\"Xception Fold \"+ str(indice) +\" \\n \\n\"\n","              \"Training and Validation Accuracy\")\n","    plt.tight_layout()\n","    plt.legend(loc='upper right')\n","    plt.grid(True, color='w', linestyle='-', linewidth=2)\n","    plt.gca().patch.set_facecolor('lightgrey')\n","    plt.legend()\n","    plt.savefig(str(output) + '/XceptionAcc_' + str(indice) + '.png')\n","    plt.show()\n","    plt.close()\n","\n","    plt.figure(1)\n","    plt.plot(xcept.history['loss'], 'b', label=\"train_loss\")\n","    plt.plot(xcept.history['val_loss'], 'm', label=\"val_loss\")\n","    plt.xlabel(\"Num of Epochs\")\n","    plt.ylabel(\"Loss\")\n","    plt.yscale('linear')\n","    plt.xscale('linear')\n","    plt.title(\"Xception Fold \"+ str(indice) +\" \\n \\n\"\n","              \"Training and Validation Loss\")\n","    plt.tight_layout()\n","    plt.legend(loc='upper right')\n","    plt.grid(True, color='w', linestyle='-', linewidth=2)\n","    plt.gca().patch.set_facecolor('lightgrey')\n","    plt.legend()\n","    plt.savefig(str(output) + '/XceptionLoss_' + str(indice) + '.png')\n","    plt.show()\n","    \n","    cm = metrics.confusion_matrix(labs_test, labs_predict)\n","    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm)\n","    disp.plot()\n","    plt.show()\n","    \n","    myXception = None\n","    imgs_test = None\n","    labs_test = None\n","    \n","    del myXception\n","    del imgs_test\n","    del labs_test\n","    gc.collect()\n","    \n","print(\"===================================================================\")\n","print(f'> Global accuracy: {np.mean(acc_folds)} {np.std(acc_folds)}')\n","print(f'> Global F1_Score:  {np.mean(f1_folds)} {np.std(f1_folds)}')\n","print(f'> Global Precision: {np.mean(prec_folds)} {np.std(prec_folds)}')\n","print(f'> Global Recall:  {np.mean(recall_folds)} {np.std(recall_folds)}')\n","print(\"===================================================================\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
